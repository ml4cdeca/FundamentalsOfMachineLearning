{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\">\n",
    "  Christopher Lüken-Winkels, Lukas Blecher\n",
    "</div>\n",
    "# Exercise 1b\n",
    "## 2 Asymptotic error of the nearest neighbor classifier for the toy example from exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "p_∞(error|X) = 1 -\\sum_{k=1} p^∗ (Y = k | X)^2 &&&& \\text{(1)}\\\\\n",
    "p^*(Y=1|X)=X&&&& \\text{(2)}\\\\\n",
    "p^*(Y=0|X)=1-X&&&& \\text{(3)}\\\\\n",
    "p(X)=1&&&& \\text{(4)}\n",
    "\\end{align}$$\n",
    "Now solve the following integral: $$p_\\infty(\\text{error})=\\int p_\\infty(\\text{error}|X)p(X)dX \\stackrel{\\text{(1)}}{=}\\int \\left( \\sum_{k=1} p^*(Y=k|X)^2 \\right)p(X)dX \\stackrel{\\text{(2),(3),(4)}}{=}\\int \\left( 1-X^2-X^2+2X-1\\right)dX= \\int_0^1 \\left(2X-2X^2\\right)dX=\\left(X^2-\\frac23X^3\\right) \\biggr\\rvert^1_0=1-\\frac23=\\frac13$$\n",
    "In exersize 1 we already approximated an error of about 33% for training sets of size 100, which confirms the theoretical solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Nearest Neighbor Classification on Real Data\n",
    "### 3.1 Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "digits = load_digits ()\n",
    "print ( digits.keys () )\n",
    "data = digits [\"data\"]\n",
    "images = digits [\"images\"]\n",
    "target = digits [\"target\"]\n",
    "target_names = digits [\"target_names\"]\n",
    "print ( data.dtype )\n",
    "#get an image of a random \"3\":\n",
    "threes=images[target==3] #could also use data[index].reshape((8,8)) \n",
    "img=threes[np.random.randint(len(threes))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert 2 == len( img.shape )\n",
    "plt.figure ()\n",
    "plt.gray ()\n",
    "plt.imshow (img , interpolation =\"bicubic\") # also try interpolation =\" bicubic \"\n",
    "plt.title('3')\n",
    "plt.show ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "X_all = data\n",
    "y_all = target\n",
    "X_train , X_test , y_train , y_test =\\\n",
    "model_selection.train_test_split ( digits.data , digits.target ,test_size = 0.4 , random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Distance function computation using loops\n",
    "$$\\text{dist:}\\ {\\rm I\\!R}^{N \\times D} \\times {\\rm I\\!R}^{M \\times D} \\rightarrow {\\rm I\\!R}^{N \\times M}$$ with $$N \\text{: Length of training set}\\\\M \\text{: Lenght of test set}\\\\D \\text{: Number of pixels} = 64$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist_loop(training, test):\n",
    "    d=[] #save distances in this matrix\n",
    "    #now: fill matrix line by line\n",
    "    for i in range(len(training)):\n",
    "        line=[] \n",
    "        #fill line for line with the distances\n",
    "        for j in range(len(test)):\n",
    "            line.append(np.sqrt(np.sum(np.square(test[j]-training[i]))))\n",
    "        d.append(line)\n",
    "    return np.array(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Distance function computation using vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist_vec(training, test):\n",
    "    #adding a new dimension to sum over (pixels) and calculate the difference to the test set in each instance\n",
    "    return np.sqrt(np.sum(np.square(training[:,None]-test),axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%timeit dist_loop(X_test,X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%timeit dist_vec(X_test,X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#check if both function even do the same\n",
    "np.array_equal(dist_vec(X_test,X_train),dist_loop(X_test,X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the vectorized solution is way faster than the looped version and gives the same result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Implement the k-nearest neighbor classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define the subset of X_train\n",
    "subX_train=X_train[(y_train==3) | (y_train==9)] #only 3's and 9's are in the subsets\n",
    "subX_test=X_test[(y_test==3) | (y_test==9)]\n",
    "subY_train=y_train[(y_train==3) | (y_train==9)]\n",
    "subY_test=y_test[(y_test==3) | (y_test==9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nearest neighbor classifier\n",
    "def NN(k,training,test,trainingLabel,testLabel):\n",
    "    #calculate the distance matrix\n",
    "    distance=dist_vec(training,test)\n",
    "    guess=[]\n",
    "    for i in range(len(test)):\n",
    "        #get indices of smallest k distances of each column\n",
    "        sorting=np.argsort(distance,axis=0).T[i][:k]\n",
    "        #for only 3's and 9's we can look at the mean and check whether it is below or above (3+9)/2=6\n",
    "        guess.append(3 if np.mean(trainingLabel[sorting])<6 else 9)\n",
    "    error=1-np.count_nonzero(np.equal(guess,testLabel))/len(testLabel)\n",
    "    return error,np.array(guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "try for $k \\in [ 1, 3, 5, 9, 17,33]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "err=[]\n",
    "for k in [1,3,5,9,17,33]:\n",
    "    errK,result=NN(k,subX_train,subX_test,subY_train,subY_test)\n",
    "    err.append(errK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot([1,3,5,9,17,33],err,'.',label='error rate')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('error')\n",
    "plt.title('error as function of k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It performs very well. Only 1 to 3 wrong classifications for all $k$'s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import random as rnd\n",
    "\n",
    "def split_folds(data, target, L):   \n",
    "    #shuffle target and data in unison\n",
    "    permutation = rnd.permutation(len(data))\n",
    "    data,target = data[permutation],target[permutation]\n",
    "    return np.array(np.array_split(data,L)),np.array(np.array_split(target,L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_val(data,target,L,k):   #only works if len(data)%L == 0\n",
    "    if len(data)%L!=0:\n",
    "        raise ValueError('Only works with denominators of the data length')\n",
    "    #start list for error values\n",
    "    Err = []\n",
    "    #split data\n",
    "    X_folds,Y_folds = split_folds(data,target,L)\n",
    "    print(Y_folds.shape)\n",
    "    #use each fold as test data once and the remaining folds as training set\n",
    "    for i in range(L):\n",
    "        X_test,Y_test = X_folds[i],Y_folds[i]\n",
    "        X_train,Y_train = np.delete(X_folds,i,axis=0),np.delete(Y_folds,i,axis=0).flatten() #training set without test set\n",
    "        X_train=np.array(list(itertools.chain(*X_train))) #combine packs to one big pack\n",
    "        err,guess = NN(k, X_train, X_test, Y_train, Y_test)\n",
    "        Err.append(err)\n",
    "    return np.mean(Err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subX_all = X_all[(y_all==3) | (y_all==9)] #only 3's and 9's are in the subsets\n",
    "suby_all = y_all[(y_all==3) | (y_all==9)]\n",
    "cross_val(subX_all,suby_all,11,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
